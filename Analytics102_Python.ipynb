{
  "cells": [
    {
      "metadata": {
        "_uuid": "382fd16a377091d05be692ecb9dc13314dc4e894",
        "collapsed": true,
        "_cell_guid": "0e75d0de-a6ac-48c7-ac60-cd0bbcaffd70",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Reading the dataset(only for kernel!!)\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\ntrain_raw = pd.read_csv(\"../input/Train_UWu5bXk.csv\")\ntest_raw = pd.read_csv(\"../input/Test_u94Q5KV.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53fb9522ae29a9a8152603624c473f7ea3331edd",
        "_cell_guid": "d3fda3ac-d280-4275-8257-c9595a0e3328",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Data Exploration & Feature Engineering ###\n\n##1. Data Exploration\n\n#Combine test and train into one file and set source to identify test or train\ntrain_raw['source']='train'\ntest_raw['source']='test'\ndata = pd.concat([train_raw, test_raw],ignore_index=True)\nprint (train_raw.shape, test_raw.shape, data.shape)\n\ndata.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e81a636e625b6971d4b871218727a111eeec9271",
        "_cell_guid": "d2502038-c16d-4d1f-b89a-db4195460632",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Check missing values:\ndata.apply(lambda x: sum(x.isnull()))\n\n## we’ll impute the missing values in Item_Weight and Outlet_Size in the data cleaning section.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f63ee91273b0be28a0f9d85e8494dc0af7575c6",
        "_cell_guid": "44298379-2440-4996-9792-70e7db6da986",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Numerical data summary:\ndata.describe()\n\n#1 Feature Engeneering:\n#Item_Visibility has a min value of zero.\n#This makes no practical sense because when a product is being sold in a store,the visibility cannot be 0.\n\n#2\n#Outlet_Establishment_Years vary from 1985 to 2009. The values might not be apt in this form. \n#Rather, if we can convert them to how old the particular store is",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fc979dd5cdce2a6fa79ecc9040dbf4b20ce82d9",
        "_cell_guid": "9ff3e6ca-e5d6-4130-b59c-e4b48db8facf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Number of unique values in each:\ndata.apply(lambda x: len(x.unique()))\n\n#This tells us that there are 1559 products and 10 outlets/stores\n# Another thing that should catch attention is that Item_Type has 16 unique values.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd3edcda9620e977d5db5a6de5e8fe6abc5b06c4",
        "_cell_guid": "15a24a2d-f2fd-4f20-91a2-2c0db4142304",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Filter categorical variables\ncategorical_columns = [x for x in data.dtypes.index if data.dtypes[x]=='object']\n#Exclude ID cols and source:\ncategorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]\n#Print frequency of categories\nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for varible %s'%col)\n    print (data[col].value_counts())\n    \n##The output gives us following observations:\n\n##Item_Fat_Content: Some of ‘Low Fat’ values mis-coded as ‘low fat’ and ‘LF’. \n##Also, some of ‘Regular’ are mentioned as ‘regular’.\n\n##Item_Type: Not all categories have substantial numbers. \n##It looks like combining them can give better results.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9bdb3d135cb96d6f42dceccaf31f2d6536c77032",
        "_cell_guid": "4975c2f7-8221-4f9e-9e97-d149ef44ad73",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## First, we need to understand the distriution of Item_Weight. We can understand it better,\n## if we can visually see it. Here, we will plot the histogram.\n%matplotlib inline \n## inline matplotlib command\n\n## plot before imputing\ndata.Item_Weight.plot(kind='hist', color='white', edgecolor='black', figsize=(10,6), title='Histogram of Item_Weight')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e25492e2c99887adf0280e5ef43014fae2baf58",
        "_cell_guid": "d8798c16-3b11-46d4-8791-a0d1952317c1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Data Cleaning ###\n\n#Determine the average weight per item:\nitem_avg_weight = data.groupby('Item_Identifier').Item_Weight.mean()\n\n#Get a boolean variable specifying missing Item_Weight values\nmiss_bool = data['Item_Weight'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('Orignal #missing: %d'% sum(miss_bool))\n\n## replace na values with the average of the item_weight for that particular product\ndata.Item_Weight.fillna(0, inplace = True)\nfor index, row in data.iterrows():\n    if(row.Item_Weight == 0):\n        data.loc[index, 'Item_Weight'] = item_avg_weight[row.Item_Identifier]\n        #print(item_avg_weight[row.Item_Identifier])\n\nprint ('Final #missing: %d'% sum(data['Item_Weight'].isnull()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b0ac6d707dc5171a4a25c0a6f3e5148b5b53dde",
        "_cell_guid": "1ea5cb51-0f25-4ebe-b8b5-9bc7e4435233",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## plot after imputing\ndata.Item_Weight.plot(kind='hist', color='white', edgecolor='black', figsize=(10,6), title='Histogram of Item_Weight')\n\n## we didnt create too much bias so this imputation is viable",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5020b7e1c613218320cea22ecd37a1b6803867ce",
        "_cell_guid": "18c4b918-5a6c-4eb8-84cf-c3af0bdc9dc5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.groupby('Outlet_Identifier').Outlet_Size.value_counts(dropna=False)\n## see that only OUT010, OUT017, OUT045 HAS NA values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9507c1c170ae632dad89f01f070a74ff963c8a9",
        "_cell_guid": "bb9457f7-e52e-48de-92a5-ca523171b10e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.groupby('Outlet_Type').Outlet_Size.value_counts(dropna=False)\n## we notice that :\n## grocery store and supermarket Type 1 has only small as the outlet size\n## so we can replace the nan with small",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e8afebb44817f0b6ef8048b216f7deffbe6b996",
        "collapsed": true,
        "_cell_guid": "00708787-7bda-411f-8722-acae1ab64c43",
        "trusted": false
      },
      "cell_type": "code",
      "source": "data.loc[data.Outlet_Identifier.isin(['OUT010','OUT017','OUT045']), 'Outlet_Size'] = 'Small'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9fa3c25f29de0128175d161be4a3932e6d585c10",
        "_cell_guid": "dc427806-049e-4286-b1cd-b016af0dc9e5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.Outlet_Size.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e090298ce60733d3dd3e03150a9db130f84d94c",
        "_cell_guid": "7b1c349d-1320-4912-827c-d0dfd7afbae9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Feature Engineering ###\n\ndata.min()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3b975f53bff8095136d92e131c665b0cb9505d96",
        "_cell_guid": "ea207030-618d-4df1-b81d-3da71a6e0849",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Notice that Item_Visibility has a minimum value of 0. It seems absurd that an item has 0 \n## visibility. Therefore, we will modify that column.\n## Here we Group by Item_Identifier, calculate mean for each group(excluding zero values), then we proceed\n## to replace the zero values in each group with the group's mean.\n\n## we have to replace 0's by na because, mean() doesnt support exclude '0' parameter \n##but it includes exclude nan parameter which is true by default\n\ndata.loc[data.Item_Visibility == 0, 'Item_Visibility'] = np.nan\n\n#aggregate by Item_Identifier\nIV_mean = data.groupby('Item_Identifier').Item_Visibility.mean()\nIV_mean\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90535eb82f03f2f481759507051af5178058603f",
        "_cell_guid": "845d4bea-a322-44cd-b7d5-3dd5f2e6fd4d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.Item_Visibility.fillna(0, inplace=True)\n\n#replace 0 values\nfor index, row in data.iterrows():\n    if(row.Item_Visibility == 0):\n        data.loc[index, 'Item_Visibility'] = IV_mean[row.Item_Identifier]\n        #print(combined.loc[index, 'Item_Visibility'])\n        \ndata.Item_Visibility.describe()\n## see that min value is not zero anymore",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61484b629f8a46010a86927500da00ef7d9afef3",
        "_cell_guid": "e5f677ee-3236-4349-bbac-051fff0a4d8c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Create a broad category of Type of Item\n\n#Get the first two characters of ID:\ndata['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n#Rename them to more intuitive categories:\ndata['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n                                                             'NC':'Non-Consumable',\n                                                             'DR':'Drinks'})\ndata['Item_Type_Combined'].value_counts()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38b3ffd269c5ae2178ea8addb5ce39351c13a507",
        "_cell_guid": "a8419660-e1c2-4178-a5b4-84f6c16222a3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Determine the years of operation of a store\n#Years:\ndata['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\ndata['Outlet_Years'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb676a12f8e94e1949826b5fad7aef8696e6268a",
        "collapsed": true,
        "_cell_guid": "31610587-99c8-4de4-acb6-462343409994",
        "trusted": false
      },
      "cell_type": "code",
      "source": " data['MRP_Factor'] = pd.cut(data.Item_MRP, [0,70,130,201,400], labels=['Low', 'Medium', 'High', 'Very High'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "830c9ae7ac46be37b1344ec1db88539a68a57abd",
        "_cell_guid": "e6e082d2-c288-4a9c-ab7f-0b24e1a4aec6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Modify categories of Item_Fat_Content\n\n#Change categories of low fat:\nprint ('Original Categories:')\nprint (data['Item_Fat_Content'].value_counts())\n\nprint ('\\nModified Categories:')\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint (data['Item_Fat_Content'].value_counts())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2ce6bddad413acbbadbe574876b34c9ce90d2bf",
        "_cell_guid": "e3e04c60-a676-46a1-b3bf-91243b3876aa",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Mark non-consumables as separate category in low_fat:\ndata.loc[data['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\ndata['Item_Fat_Content'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "750b7f6697652b298533e9bde8550875e659a0c6",
        "collapsed": true,
        "_cell_guid": "333a9ffe-dad9-4f61-8518-217f9a1b7836",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\ndata['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\nvar_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet', 'MRP_Factor']\nle = LabelEncoder()\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbae033a508fde3c740463601de4739b961eb3f4",
        "collapsed": true,
        "_cell_guid": "5b2ea731-3d9c-4dde-b577-327e23b5d4d2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#One Hot Coding: dummy varriables\n\ndata = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',\n                              'Item_Type_Combined','Outlet', 'MRP_Factor'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ee79cce5a8cb095a27294780044492c91d95627",
        "_cell_guid": "11926c1f-983c-4c9f-9075-64cbf194fb5a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.dtypes\n#Here we can see that all variables are now float and each category has a new variable.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f177972b1d02819d698712fe265534b17187c7a6",
        "_cell_guid": "96fb2b96-fa7e-41f9-ae0c-13f4d76dea09",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data[['Item_Fat_Content_0','Item_Fat_Content_1','Item_Fat_Content_2']].head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c1f9c37622c87d11a7c02f55c709de34d748f9e",
        "_cell_guid": "a38fec34-aee5-423b-b524-2d69096b9e4f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Exporting Data ###\n#Drop the columns which have been converted to different types:\ndata.drop(['Item_Type','Outlet_Establishment_Year',],axis=1,inplace=True)\n\n#Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n\n#Export files as modified versions:\n#train.to_csv(\"train_modified.csv\",index=False)\n#test.to_csv(\"test_modified.csv\",index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f19fbe4147f0c812ae6cce17e744de4977368187",
        "_cell_guid": "f853bf41-2d22-4917-acf3-91a93f81a06a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28394c902474bd86582ca0bda8f508b255c5a513",
        "_cell_guid": "d2ed11e9-f2a9-4fb4-b7bb-c69c19aa819a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d11fbd1480e06554ca394e50ca6cfa9bd73e4e33",
        "_cell_guid": "d2114380-2af7-447e-b847-4c8c3e4831ab",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## lets draw some plots to see that the regression assumptions are not voilated\n## QQ plot\n\nimport pylab \nimport scipy.stats as stats\n\nquantile = train.Item_Outlet_Sales\n\nstats.probplot(quantile, dist=\"uniform\", plot=pylab)\npylab.show()\n\n## the line is almost linear except for the end points ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d993d8b7d1c8d175589171668a31a0d869bf6293",
        "_cell_guid": "ad0523ca-abc1-4d10-9b48-002193c13bcd",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Model Building ###\n\n#Define target and ID columns:\n\n##Since I’ll be making many models, instead of repeating the codes again and again, \n##I would like to define a generic function which takes the algorithm and data as input and makes the model\n##performs cross-validation and generates submission\n\n# we want to predict target\ntarget = 'Item_Outlet_Sales'\n\n#below are just identifiers which we dont want to fit\nIDcol = ['Item_Identifier','Outlet_Identifier']\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate, cross_val_score\nimport matplotlib.pyplot as plt\n\ndef modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename, resid=False, transform=False):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    \n    residuals = dtrain_predictions - dtrain[target]\n    if(transform == True):\n        train_mod = train.copy(deep = True)\n        train_mod[target] = train_mod[target].apply(np.log)\n        dtrain_predictions = np.exp(dtrain_predictions)\n        #print(dtrain_predictions)\n\n    \n    #residuals vs fitted plot\n    if(resid == True):\n        plt.scatter(dtrain_predictions, residuals)\n        plt.xlabel('fitted values')\n        plt.ylabel('residuals')\n        plt.show()\n    \n    #Perform cross-validation:\n    cv_score = cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(np.abs(cv_score))\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n    print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    dtest[target] = alg.predict(dtest[predictors])\n    \n    #Export submission file:\n    IDcol.append(target)\n    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n    submission.to_csv(filename, index=False)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dada714380b94bd0dced01cd5852c82e1d70906d",
        "_cell_guid": "de966c74-2309-436c-abb9-37981ae41a8f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "### Linear Regression Model\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\npredictors = [x for x in train.columns if x not in [target]+IDcol]\n# print predictors\nalg1 = LinearRegression(normalize=True)\npred1 = np.nan\nmodelfit(alg1, train, test, predictors, target, IDcol, 'alg1.csv', resid=True)\n\n\ncoef1 = pd.Series(alg1.coef_, predictors).sort_values()\ncoef1.plot(kind='bar', title='Model Coefficients', figsize=(10,6))\n\n#if you notice the coefficients, they are very large in magnitude which signifies overfitting. \n#To cater to this, we will use a ridge regression model.\n\n## residual vs fitted plot and model coefficients plot is given below",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66629f33e18e28ebff73119aff0f9c9b33097a73",
        "_cell_guid": "e1bcfb7e-d0fd-4b52-b37d-5594deae5c1f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## As the residual vs fitted plot is funnel shaped, \n## the response variable suffers from non-constant variance\n##we can do a log transformation, square root trasformation on the response variable\n## to make it linear and to improve the model even further",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "44512a5d46a728e853421baf9a074be5e64d93cb",
        "_cell_guid": "44a8fd85-4504-4d6a-9293-d804117bffd1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Ridge Regression Model:\n\n## lets take alpha 0.05 for now for both Ridge and Lasso\n\nalg2 = Ridge(alpha=0.05,normalize=True)\nmodelfit(alg2, train, test, predictors, target, IDcol, 'alg2.csv')\n\n#The regression coefficient got better now, also the cross validation score has improved\n##bot the rmse didnt change much",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd9281aedf6c1b38be06b9ccf95ac71792bf6dd1",
        "_cell_guid": "e7c98d32-824b-4109-9e51-12ec0b19a965",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Lasso Regression Model:\n\nalg3 = Lasso(alpha=0.05,normalize=True)\nmodelfit(alg3, train, test, predictors, target, IDcol, 'alg3.csv')\ncoef3 = pd.Series(alg3.coef_, predictors).sort_values()\ncoef3.plot(kind='bar', title='Model Coefficients', figsize=(10,6))\n\n## you can see that the coefficients of some columns have decreased but for some variables the \n## coeffiecients has almost doubled than that of Ridge regression\n## also the mean cross validation score has increased in comparison to Ridge\n## RMSE didnt change much",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c96fbac9117bb41bef2e79a0578820787ccc1c8",
        "collapsed": true,
        "_cell_guid": "b5d0c74e-3c2f-491b-9737-e58877d3965a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "## comparing the cross validation score for all three models Ridge regression has the lowest mean score\n## and lowest model coefficients for all columns\n## RMSE for all the models were almost same\n\n## you can run a 'for' loop for alpha between 1 and 20 to see if the model improves\n## the cross validation and other metrics and choose the best model ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}